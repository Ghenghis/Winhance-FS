"""
GPU Accelerator for RTX 3090 Ti

Leverages CUDA for:
- Fast file hashing
- Parallel embedding generation
- Image similarity computation
- Model inference acceleration
"""

from __future__ import annotations

import os
import threading
from dataclasses import dataclass
from pathlib import Path
from typing import Any

from nexus_ai.core.logging_config import get_logger

logger = get_logger("gpu_accelerator")


@dataclass
class GPUInfo:
    """GPU device information."""

    device_id: int
    name: str
    compute_capability: tuple
    total_memory_gb: float
    free_memory_gb: float
    is_available: bool


@dataclass
class GPUConfig:
    """GPU acceleration configuration."""

    enabled: bool = True
    device_id: int = 0
    memory_fraction: float = 0.9
    batch_size: int = 32
    num_workers: int = 4
    use_fp16: bool = True
    use_tensor_cores: bool = True


class GPUAccelerator:
    """
    GPU acceleration manager for file operations.

    Optimized for NVIDIA RTX 3090 Ti:
    - 24GB VRAM
    - 10496 CUDA cores
    - Tensor cores for mixed precision
    """

    def __init__(self, config: GPUConfig | None = None):
        self.config = config or GPUConfig()
        self._torch = None
        self._cuda_available = False
        self._device = None
        self._gpu_info: GPUInfo | None = None
        self._lock = threading.Lock()

        self._initialize()

    def _initialize(self) -> None:
        """Initialize GPU resources."""
        try:
            import torch

            self._torch = torch

            if torch.cuda.is_available() and self.config.enabled:
                self._cuda_available = True
                self._device = torch.device(f"cuda:{self.config.device_id}")

                # Set memory fraction
                torch.cuda.set_per_process_memory_fraction(
                    self.config.memory_fraction, self.config.device_id
                )

                # Get GPU info
                props = torch.cuda.get_device_properties(self.config.device_id)
                total_mem = props.total_memory / (1024**3)
                free_mem = torch.cuda.memory_reserved(self.config.device_id) / (1024**3)

                self._gpu_info = GPUInfo(
                    device_id=self.config.device_id,
                    name=props.name,
                    compute_capability=(props.major, props.minor),
                    total_memory_gb=total_mem,
                    free_memory_gb=total_mem - free_mem,
                    is_available=True,
                )

                logger.info(
                    f"GPU initialized: {self._gpu_info.name}",
                    memory_gb=self._gpu_info.total_memory_gb,
                    compute_capability=self._gpu_info.compute_capability,
                )
            else:
                logger.warning("CUDA not available, using CPU fallback")
                self._device = torch.device("cpu") if torch else None

        except ImportError:
            logger.warning("PyTorch not installed, GPU acceleration disabled")
            self._cuda_available = False

    @property
    def is_available(self) -> bool:
        """Check if GPU acceleration is available."""
        return self._cuda_available

    @property
    def gpu_info(self) -> GPUInfo | None:
        """Get GPU information."""
        return self._gpu_info

    def get_memory_stats(self) -> dict[str, float]:
        """Get current GPU memory statistics."""
        if not self._cuda_available or not self._torch:
            return {"error": "GPU not available"}

        torch = self._torch
        return {
            "allocated_gb": torch.cuda.memory_allocated(self.config.device_id) / (1024**3),
            "reserved_gb": torch.cuda.memory_reserved(self.config.device_id) / (1024**3),
            "max_allocated_gb": torch.cuda.max_memory_allocated(self.config.device_id) / (1024**3),
        }

    async def compute_embeddings_batch(
        self,
        texts: list[str],
        model_name: str = "all-MiniLM-L6-v2",
    ) -> list[list[float]]:
        """
        Compute embeddings for a batch of texts using GPU.

        Args:
            texts: List of text strings to embed
            model_name: Sentence transformer model name

        Returns:
            List of embedding vectors
        """
        if not self._cuda_available:
            logger.warning("GPU not available for embeddings, using CPU")

        try:
            from sentence_transformers import SentenceTransformer

            # Load model with GPU support
            device = "cuda" if self._cuda_available else "cpu"
            model = SentenceTransformer(model_name, device=device)

            # Compute embeddings in batches
            embeddings = model.encode(
                texts,
                batch_size=self.config.batch_size,
                show_progress_bar=False,
                convert_to_numpy=True,
            )

            return embeddings.tolist()

        except ImportError:
            logger.error("sentence-transformers not installed")
            return []

    async def compute_image_hashes_batch(
        self,
        image_paths: list[Path],
    ) -> dict[str, str]:
        """
        Compute perceptual hashes for images using GPU.

        Args:
            image_paths: List of image file paths

        Returns:
            Dict mapping path to hash
        """
        results = {}

        try:
            import imagehash
            from PIL import Image

            for path in image_paths:
                try:
                    img = Image.open(path)
                    # Perceptual hash (good for finding similar images)
                    phash = imagehash.phash(img)
                    results[str(path)] = str(phash)
                except Exception as e:
                    logger.debug(f"Failed to hash image {path}: {e}")

        except ImportError:
            logger.error("PIL/imagehash not installed for image hashing")

        return results

    def accelerate_hash_computation(
        self,
        file_paths: list[Path],
        algorithm: str = "xxhash",
    ) -> dict[str, str]:
        """
        Accelerate file hashing using GPU/multi-threading.

        Note: Pure file hashing is I/O bound, but we parallelize reading.
        """
        import hashlib
        from concurrent.futures import ThreadPoolExecutor

        results = {}

        def hash_file(path: Path) -> tuple:
            try:
                if algorithm == "xxhash":
                    try:
                        import xxhash

                        hasher = xxhash.xxh64()
                    except ImportError:
                        hasher = hashlib.md5()
                else:
                    hasher = hashlib.new(algorithm)

                with open(path, "rb") as f:
                    while chunk := f.read(65536):
                        hasher.update(chunk)

                return str(path), hasher.hexdigest()
            except Exception as e:
                logger.debug(f"Failed to hash {path}: {e}")
                return str(path), None

        with ThreadPoolExecutor(max_workers=self.config.num_workers) as executor:
            for path, hash_val in executor.map(hash_file, file_paths):
                if hash_val:
                    results[path] = hash_val

        return results

    async def run_local_inference(
        self,
        prompt: str,
        model_path: Path | None = None,
        max_tokens: int = 512,
    ) -> str:
        """
        Run local model inference with GPU acceleration.

        Uses llama-cpp-python with CUDA support.
        """
        if not self._cuda_available:
            logger.warning("GPU not available for inference")

        try:
            from llama_cpp import Llama

            # Find default model if not specified
            if model_path is None:
                model_dirs = [
                    Path.home() / ".lmstudio" / "models",
                    Path.home() / ".ollama" / "models",
                ]
                for dir in model_dirs:
                    if dir.exists():
                        gguf_files = list(dir.rglob("*.gguf"))
                        if gguf_files:
                            model_path = gguf_files[0]
                            break

            if model_path is None or not model_path.exists():
                return "Error: No model found"

            # Load with GPU layers
            llm = Llama(
                model_path=str(model_path),
                n_gpu_layers=-1 if self._cuda_available else 0,
                n_ctx=4096,
                verbose=False,
            )

            # Generate
            output = llm(
                prompt,
                max_tokens=max_tokens,
                temperature=0.7,
                echo=False,
            )

            return output["choices"][0]["text"]

        except ImportError:
            logger.error("llama-cpp-python not installed")
            return "Error: llama-cpp-python not installed"
        except Exception as e:
            logger.error(f"Inference error: {e}")
            return f"Error: {e}"

    def optimize_for_rtx3090(self) -> None:
        """Apply RTX 3090 Ti specific optimizations."""
        if not self._cuda_available or not self._torch:
            return

        torch = self._torch

        # Enable TF32 for Ampere GPUs (RTX 30 series)
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True

        # Enable cudnn benchmark for consistent workloads
        torch.backends.cudnn.benchmark = True

        # Set memory allocator for better fragmentation handling
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"

        logger.info("Applied RTX 3090 Ti optimizations")

    def clear_cache(self) -> None:
        """Clear GPU memory cache."""
        if self._cuda_available and self._torch:
            self._torch.cuda.empty_cache()
            logger.info("GPU cache cleared")


# Global GPU accelerator instance
_gpu_accelerator: GPUAccelerator | None = None


def get_gpu_accelerator() -> GPUAccelerator:
    """Get the global GPU accelerator instance."""
    global _gpu_accelerator
    if _gpu_accelerator is None:
        _gpu_accelerator = GPUAccelerator()
        if _gpu_accelerator.is_available:
            _gpu_accelerator.optimize_for_rtx3090()
    return _gpu_accelerator


def check_gpu_status() -> dict[str, Any]:
    """Quick check of GPU status."""
    acc = get_gpu_accelerator()
    if acc.gpu_info:
        return {
            "available": True,
            "name": acc.gpu_info.name,
            "memory_gb": acc.gpu_info.total_memory_gb,
            "compute_capability": acc.gpu_info.compute_capability,
            "stats": acc.get_memory_stats(),
        }
    return {"available": False}
